p8105\_hw6\_jl6048
================
Jinghan Liu

``` r
library(tidyverse)
library(readxl)
library(modelr)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%")
```

### Problem 1

Load and clean the dataset for regression analysis:

``` r
child_df = 
  readr::read_csv("data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
    malform = as.factor(malform)
  )
```

    ## Rows: 4342 Columns: 20

    ## -- Column specification --------------------------------------------------------
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...

    ## 
    ## i Use `spec()` to retrieve the full column specification for this data.
    ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# check for missing value - no missing
sum(complete.cases(child_df))
```

    ## [1] 4342

``` r
sum(!complete.cases(child_df))
```

    ## [1] 0

Propose a regression model for birthweight:

``` r
model_1 = lm(bwt ~ gaweeks, data = child_df)
summary(model_1)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ gaweeks, data = child_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1730.52  -292.85    -0.78   303.47  1591.36 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  476.003     88.809    5.36 8.76e-08 ***
    ## gaweeks       66.920      2.245   29.80  < 2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 466.7 on 4340 degrees of freedom
    ## Multiple R-squared:  0.1699, Adjusted R-squared:  0.1697 
    ## F-statistic: 888.3 on 1 and 4340 DF,  p-value: < 2.2e-16

A plot of model residuals against fitted values:

``` r
child_df %>% 
  modelr::add_residuals(model_1) %>% 
  modelr::add_predictions(model_1) %>% 
  ggplot(aes(x = pred, y = resid, color = resid)) + 
  geom_point(alpha = 0.5)+ 
  stat_smooth(method = "lm") +
  labs(title = "Predicted vs residuals", 
       x = "Predicted", 
       y = "Residuals")
```

    ## `geom_smooth()` using formula 'y ~ x'

<img src="p8105_hw6_jl6048_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

**Describe:** model\_1 is a simple linear regression with gestational
age as the predictor (X) and birth weight as Y. It can be seen from the
plot that the residuals are relatively evenly distributed around y = 0
and below. Therefore, it satisfies the linearity assumption.

Compare your model to two others:

``` r
# One using length at birth and gestational age as predictors (main effects only)
model_2 = lm(bwt ~ blength + gaweeks, data = child_df) %>%
  broom::glance()


#One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
model_3 = lm(bwt ~bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex , data = child_df) %>%
  broom::glance()
```

Make this comparison in terms of the cross-validated prediction error:

``` r
cv_data =
  crossv_mc(child_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
cv_data 
```

    ## # A tibble: 100 x 3
    ##    train                 test                .id  
    ##    <list>                <list>              <chr>
    ##  1 <tibble [3,473 x 20]> <tibble [869 x 20]> 001  
    ##  2 <tibble [3,473 x 20]> <tibble [869 x 20]> 002  
    ##  3 <tibble [3,473 x 20]> <tibble [869 x 20]> 003  
    ##  4 <tibble [3,473 x 20]> <tibble [869 x 20]> 004  
    ##  5 <tibble [3,473 x 20]> <tibble [869 x 20]> 005  
    ##  6 <tibble [3,473 x 20]> <tibble [869 x 20]> 006  
    ##  7 <tibble [3,473 x 20]> <tibble [869 x 20]> 007  
    ##  8 <tibble [3,473 x 20]> <tibble [869 x 20]> 008  
    ##  9 <tibble [3,473 x 20]> <tibble [869 x 20]> 009  
    ## 10 <tibble [3,473 x 20]> <tibble [869 x 20]> 010  
    ## # ... with 90 more rows

``` r
cv_data = 
  cv_data %>% 
  mutate(
    model_1  = map(train, ~lm(bwt ~ gaweeks, data = .x)),
    model_2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_3  = map(train, ~lm(bwt ~ 
       bhead + blength + babysex + 
       bhead * blength + 
       bhead * babysex + 
       blength * babysex + 
       bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_model_1 = 
      map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = 
      map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_model_3 =
      map2_dbl(model_3, test, ~rmse(model = .x, data = .y)))
cv_data
```

    ## # A tibble: 100 x 9
    ##    train                 test  .id   model_1 model_2 model_3 rmse_model_1 rmse_model_2
    ##    <list>                <lis> <chr> <list>  <list>  <list>         <dbl>        <dbl>
    ##  1 <tibble [3,473 x 20]> <tib~ 001   <lm>    <lm>    <lm>            460.         339.
    ##  2 <tibble [3,473 x 20]> <tib~ 002   <lm>    <lm>    <lm>            479.         339.
    ##  3 <tibble [3,473 x 20]> <tib~ 003   <lm>    <lm>    <lm>            482.         365.
    ##  4 <tibble [3,473 x 20]> <tib~ 004   <lm>    <lm>    <lm>            484.         320.
    ##  5 <tibble [3,473 x 20]> <tib~ 005   <lm>    <lm>    <lm>            453.         318.
    ##  6 <tibble [3,473 x 20]> <tib~ 006   <lm>    <lm>    <lm>            472.         330.
    ##  7 <tibble [3,473 x 20]> <tib~ 007   <lm>    <lm>    <lm>            473.         340.
    ##  8 <tibble [3,473 x 20]> <tib~ 008   <lm>    <lm>    <lm>            456.         350.
    ##  9 <tibble [3,473 x 20]> <tib~ 009   <lm>    <lm>    <lm>            481.         338.
    ## 10 <tibble [3,473 x 20]> <tib~ 010   <lm>    <lm>    <lm>            464.         317.
    ## # ... with 90 more rows, and 1 more variable: rmse_model_3 <dbl>

``` r
cv_data %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
   labs(
        title = "Comparing birthweight models",
        x = "Model",
        y = "Rmse"
      )
```

<img src="p8105_hw6_jl6048_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

**Comment:** From this violin plot, it can be seen that the Rmse of the
first model is the highest, and the Rmse of the third model is the
lowest. The third model with interaction terms is the best model because
it has the lowest mean square value.

### Problem 2:

Load data

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## using cached file: C:\Users\liuji\AppData\Local/Cache/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2021-12-02 13:34:02 (7.629)

    ## file min/max dates: 1869-01-01 / 2021-11-30

``` r
weather_df 
```

    ## # A tibble: 365 x 6
    ##    name           id          date        prcp  tmax  tmin
    ##    <chr>          <chr>       <date>     <dbl> <dbl> <dbl>
    ##  1 CentralPark_NY USW00094728 2017-01-01     0   8.9   4.4
    ##  2 CentralPark_NY USW00094728 2017-01-02    53   5     2.8
    ##  3 CentralPark_NY USW00094728 2017-01-03   147   6.1   3.9
    ##  4 CentralPark_NY USW00094728 2017-01-04     0  11.1   1.1
    ##  5 CentralPark_NY USW00094728 2017-01-05     0   1.1  -2.7
    ##  6 CentralPark_NY USW00094728 2017-01-06    13   0.6  -3.8
    ##  7 CentralPark_NY USW00094728 2017-01-07    81  -3.2  -6.6
    ##  8 CentralPark_NY USW00094728 2017-01-08     0  -3.8  -8.8
    ##  9 CentralPark_NY USW00094728 2017-01-09     0  -4.9  -9.9
    ## 10 CentralPark_NY USW00094728 2017-01-10     0   7.8  -6  
    ## # ... with 355 more rows

For R squared:

``` r
bootstrap_r =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm( tmax ~ tmin , data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(.id, results) %>% 
  unnest(results)
bootstrap_r
```

    ## # A tibble: 5,000 x 13
    ##    .id   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC
    ##    <chr>     <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl>
    ##  1 0001      0.925         0.925  2.87     4503. 1.10e-206     1  -901. 1809.
    ##  2 0002      0.916         0.916  2.84     3950. 3.56e-197     1  -898. 1803.
    ##  3 0003      0.915         0.915  2.86     3900. 2.92e-196     1  -900. 1806.
    ##  4 0004      0.906         0.906  3.03     3510. 1.08e-188     1  -922. 1849.
    ##  5 0005      0.918         0.918  2.90     4056. 4.39e-199     1  -905. 1816.
    ##  6 0006      0.919         0.919  2.98     4141. 1.34e-200     1  -915. 1837.
    ##  7 0007      0.910         0.910  2.99     3663. 9.77e-192     1  -917. 1840.
    ##  8 0008      0.909         0.909  2.98     3618. 7.26e-191     1  -915. 1836.
    ##  9 0009      0.912         0.912  2.92     3768. 9.08e-194     1  -908. 1821.
    ## 10 0010      0.921         0.921  2.82     4223. 5.19e-202     1  -895. 1795.
    ## # ... with 4,990 more rows, and 4 more variables: BIC <dbl>, deviance <dbl>,
    ## #   df.residual <int>, nobs <int>

``` r
bootstrap_r %>% 
ggplot(aes(x = r.squared)) + 
  geom_density() +
   labs(
      x = "R squared values",
      y = "Density",
      title = "Distribution of R Squared Estimates")
```

<img src="p8105_hw6_jl6048_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

For log of betas:

``` r
bootstrap_log =
  weather_df %>% 
   modelr::bootstrap(n = 5000)%>% 
  mutate(
    models = map(strap, ~lm( tmax ~ tmin , data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(.id, results) %>% 
  unnest(results)%>% 
  select(.id, term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    beta_log = log(intercept * tmin)
  ) 
bootstrap_log
```

    ## # A tibble: 5,000 x 4
    ##    id    intercept  tmin beta_log
    ##    <chr>     <dbl> <dbl>    <dbl>
    ##  1 0001       7.27  1.04     2.02
    ##  2 0002       7.01  1.06     2.00
    ##  3 0003       7.02  1.05     1.99
    ##  4 0004       7.27  1.03     2.02
    ##  5 0005       7.70  1.03     2.07
    ##  6 0006       6.95  1.05     1.98
    ##  7 0007       7.51  1.02     2.04
    ##  8 0008       7.19  1.05     2.02
    ##  9 0009       7.29  1.03     2.02
    ## 10 0010       7.42  1.03     2.03
    ## # ... with 4,990 more rows

``` r
bootstrap_log %>% 
ggplot(aes(x = beta_log)) + 
  geom_density() +
   labs(
      x = "Log coefficients values",
      y = "Density",
      title = "Distribution of log coefficients Estimates")
```

<img src="p8105_hw6_jl6048_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />
**Comment:** From the distribution plot, we can easily find that the
distribution of R Squared looks like a normal distribution but slightly
left skewed. The log plot looks like more skewed to the normal
distribution than R.

Confidence Interval :

``` r
bootstrap_r %>% 
  summarize(
    CI_lower_r = quantile(r.squared, 0.025),
    CI_upper_r = quantile(r.squared, 0.975)
  ) %>% 
  knitr::kable(digits = 3)
```

| CI\_lower\_r | CI\_upper\_r |
|-------------:|-------------:|
|        0.894 |        0.927 |

``` r
bootstrap_log %>% 
  summarize(
    CI_lower_log = quantile(beta_log, 0.025),
    CI_upper_log = quantile(beta_log, 0.975)
  ) %>% 
  knitr::kable(digits = 3)
```

| CI\_lower\_log | CI\_upper\_log |
|---------------:|---------------:|
|          1.965 |          2.059 |
